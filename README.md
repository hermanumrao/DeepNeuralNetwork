Performing back-propagation on a neural network involves calculating the gradients of the loss function with respect to each weight and bias in the network and then updating these weights and biases using gradient descent. Below, I'll outline the steps and formulas for back-propagation and gradient descent for a simple neural network with:

- 1 input layer with 2 neurons
- 1 hidden layer with 2 neurons
- 1 output layer with 1 neuron
  
  
  ![](ImgDump/NN_image.png)
  
  
  
  

![](ImgDump/Screenshot%20from%202024-06-07%2022-50-57.png)

![](ImgDump/Screenshot%20from%202024-06-07%2022-51-28.png)

![](ImgDump/Screenshot%20from%202024-06-07%2022-51-36.png)

![](ImgDump/Screenshot%20from%202024-06-07%2022-51-57.png)

![](ImgDump/Screenshot%20from%202024-06-07%2022-52-16.png)
